{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3fee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"GMMExample\").getOrCreate()\n",
    "\n",
    "# Read CSV data from the data folder\n",
    "df = spark.read.csv(\"../data/Thursday.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Select current columns plus an additional label column for testing\n",
    "columns_to_keep = [\n",
    "    \"protocol\",\n",
    "    \"src2dst_packets\",\n",
    "    \"dst2src_packets\",\n",
    "    \"src2dst_bytes\",\n",
    "    \"dst2src_bytes\",\n",
    "    \"bidirectional_duration_ms\",\n",
    "    \"bidirectional_min_ps\",\n",
    "    \"bidirectional_max_ps\",\n",
    "    \"bidirectional_mean_ps\",\n",
    "    \"bidirectional_stddev_ps\",\n",
    "    \"src2dst_max_ps\",\n",
    "    \"src2dst_min_ps\",\n",
    "    \"src2dst_mean_ps\",\n",
    "    \"src2dst_stddev_ps\",\n",
    "    \"dst2src_max_ps\",\n",
    "    \"dst2src_min_ps\",\n",
    "    \"dst2src_mean_ps\",\n",
    "    \"dst2src_stddev_ps\",\n",
    "    \"bidirectional_mean_piat_ms\",\n",
    "    \"bidirectional_stddev_piat_ms\",\n",
    "    \"bidirectional_max_piat_ms\",\n",
    "    \"bidirectional_min_piat_ms\",\n",
    "    \"src2dst_mean_piat_ms\",\n",
    "    \"src2dst_stddev_piat_ms\",\n",
    "    \"src2dst_max_piat_ms\",\n",
    "    \"src2dst_min_piat_ms\",\n",
    "    \"dst2src_mean_piat_ms\",\n",
    "    \"dst2src_stddev_piat_ms\",\n",
    "    \"dst2src_max_piat_ms\",\n",
    "    \"dst2src_min_piat_ms\",\n",
    "    \"bidirectional_fin_packets\",\n",
    "    \"bidirectional_syn_packets\",\n",
    "    \"bidirectional_rst_packets\",\n",
    "    \"bidirectional_psh_packets\",\n",
    "    \"bidirectional_ack_packets\",\n",
    "    \"bidirectional_urg_packets\",\n",
    "    \"bidirectional_cwr_packets\",\n",
    "    \"bidirectional_ece_packets\",\n",
    "    \"src2dst_psh_packets\",\n",
    "    \"dst2src_psh_packets\",\n",
    "    \"src2dst_urg_packets\",\n",
    "    \"dst2src_urg_packets\",\n",
    "    # Add your label column here (e.g. \"label\" or \"attack_type\")\n",
    "    \"label\"\n",
    "]\n",
    "\n",
    "df = df.select([col(c) for c in columns_to_keep if c in df.columns])\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56a7231",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3febdd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the barchart of the number of samples in each class\n",
    "if \"label\" in df.columns:\n",
    "    class_counts = df.groupBy(\"label\").count().toPandas()\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(class_counts[\"label\"].astype(str), class_counts[\"count\"])\n",
    "    plt.xlabel(\"Class Label\")\n",
    "    plt.ylabel(\"Number of Samples\")\n",
    "    plt.title(\"Number of Samples in Each Class\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No label column found for class visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb052b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Encode label: BENIGN as 0, all others as 1 (ANOMALY)\n",
    "if \"label\" in df.columns:\n",
    "    df = df.withColumn(\"label\", when(col(\"label\") == \"BENIGN\", \"BENIGN\").otherwise(\"ANOMALY\"))\n",
    "    df.groupBy(\"label\").count().show()\n",
    "else:\n",
    "    print(\"No label column found for encoding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acd8e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Cleaning (drop missing values)\n",
    "from pyspark.sql.functions import isnan, isnull\n",
    "\n",
    "nan_condition = None\n",
    "for c in df.columns:\n",
    "    cond = isnull(col(c)) | isnan(col(c))\n",
    "    nan_condition = cond if nan_condition is None else nan_condition | cond\n",
    "\n",
    "df.filter(nan_condition).show(5, truncate=False)\n",
    "\n",
    "df_clean = df.dropna()\n",
    "print(\"Rows after dropna():\", df_clean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777fef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Data Encoding (categorical to numeric)\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Exclude 'label' from categorical_cols for encoding as it is the target\n",
    "categorical_cols = [field.name for field in df_clean.schema.fields if isinstance(field.dataType, StringType) and field.name != \"label\"]\n",
    "numeric_cols = [field.name for field in df_clean.schema.fields if field.dataType.typeName() in ['integer', 'double', 'long', 'float']]\n",
    "\n",
    "for col_name in categorical_cols:\n",
    "    indexer = StringIndexer(inputCol=col_name, outputCol=col_name + \"_idx\").fit(df_clean)\n",
    "    df_clean = indexer.transform(df_clean)\n",
    "\n",
    "# Encode label column if it's string type\n",
    "if \"label\" in [f.name for f in df_clean.schema.fields]:\n",
    "    label_field = [f for f in df_clean.schema.fields if f.name == \"label\"][0]\n",
    "    if isinstance(label_field.dataType, StringType):\n",
    "        label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_idx\").fit(df_clean)\n",
    "        df_clean = label_indexer.transform(df_clean)\n",
    "        label_col = \"label_idx\"\n",
    "    else:\n",
    "        label_col = \"label\"\n",
    "else:\n",
    "    label_col = None\n",
    "\n",
    "feature_cols = [col + \"_idx\" for col in categorical_cols] + numeric_cols\n",
    "df_encoded = df_clean\n",
    "df_encoded.select(feature_cols + ([label_col] if label_col else [])).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a974e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Data Normalization (scaling)\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_vec\")\n",
    "df_features = assembler.transform(df_encoded)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"features_scaled\")\n",
    "scaler_model = scaler.fit(df_features)\n",
    "df_scaled = scaler_model.transform(df_features)\n",
    "\n",
    "df_scaled.select(\"features_scaled\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba29c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Train/Test Gaussian Mixture Model\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "\n",
    "# Use only the scaled features and label for evaluation\n",
    "if label_col:\n",
    "    data = df_scaled.select(\"features_scaled\", label_col).withColumnRenamed(\"features_scaled\", \"features\").withColumnRenamed(label_col, \"label\")\n",
    "else:\n",
    "    data = df_scaled.select(\"features_scaled\").withColumnRenamed(\"features_scaled\", \"features\")\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Train GMM\n",
    "gmm = GaussianMixture(featuresCol=\"features\", k=2, seed=42)\n",
    "model = gmm.fit(train_data)\n",
    "\n",
    "# Predict clusters for test data\n",
    "predictions = model.transform(test_data)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418d9414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Ensure prediction column is double type for evaluator compatibility\n",
    "if \"label\" in predictions.columns:\n",
    "    predictions = predictions.withColumn(\"prediction\", col(\"prediction\").cast(\"double\"))\n",
    "    predictions = predictions.withColumn(\"label\", col(\"label\").cast(\"double\"))\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "    f1 = evaluator.evaluate(predictions)\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "else:\n",
    "    print(\"No label column found; skipping classification metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642898c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Use the same train/test data as before\n",
    "if \"label\" in train_data.columns:\n",
    "    dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\", seed=42)\n",
    "    dt_model = dt.fit(train_data)\n",
    "    dt_predictions = dt_model.transform(test_data)\n",
    "    dt_predictions.show(5)\n",
    "\n",
    "    # Evaluate Decision Tree model\n",
    "    dt_predictions = dt_predictions.withColumn(\"prediction\", col(\"prediction\").cast(\"double\"))\n",
    "    dt_predictions = dt_predictions.withColumn(\"label\", col(\"label\").cast(\"double\"))\n",
    "    dt_accuracy = evaluator.evaluate(dt_predictions)\n",
    "    print(f\"Decision Tree Accuracy: {dt_accuracy}\")\n",
    "\n",
    "    dt_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\").evaluate(dt_predictions)\n",
    "    print(f\"Decision Tree F1 Score: {dt_f1}\")\n",
    "else:\n",
    "    print(\"No label column found; skipping Decision Tree classification.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
